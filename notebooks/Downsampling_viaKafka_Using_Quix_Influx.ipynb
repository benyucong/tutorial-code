{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Continuously downsample data using InfluxDB and Quix Streams\n",
        "This notebook allows you to run a real time data processing pipeline that downsamples data sourced from InfluxDB then writes it back in again.\n",
        "\n",
        "* The data processing uses the [Quix Streams library](https://github.com/quixio/quix-streams) as the processing engine\n",
        "* Quix Streams is an open source Python library for building containerized stream processing applications with Apache Kafka.\n",
        "* Quix Streams is designed to run continuously on a stream of data, so to run it in Google Colab, we'll run the processes in the background."
      ],
      "metadata": {
        "id": "4rKEW-M5AE33"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B6KMYC1gEpMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT1cmV4qCa2X"
      },
      "source": [
        "<br>![downsample-2024-02-19-2312.png](quix-pipeline-downsample.png)"]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook sections:\n",
        "\n",
        "1. **Install the dependencies**\n",
        "2. **Generate synthetic machine data (optional)**\n",
        "3. **Read the raw data from InfluxDB**\n",
        "4. <font color=\"blue\">**Downsample the data**</font>\n",
        "5. **Insert the downsampled data back into InfluxDB**\n",
        "\n",
        "**Note:**\n",
        "* If you just want to see the downsampling code, head straight to **Section 4**—the downsampling section.\n",
        "\n",
        "* If you want to run this kind of pipeline in production, check out [Quix Cloud](https://quix.io/product) where you can run Quix Streams processes in a scalable, fault-tolerant way."
      ],
      "metadata": {
        "id": "1aVBJDBmAKop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Install the Quixstreams, the InfluxDB client and Apache Kafka, then start the Kafka servers."
      ],
      "metadata": {
        "id": "Jr0T6gYFutcO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPYtfAR_9YxZ"
      },
      "source": [
        "### 1. Install the main dependencies\n",
        "\n",
        "Dependencies include:the Quix Streams library, Qdrant library, and the sentence transformers library (we'll use the default sentence transformers embedding model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPbus2gi61ze"
      },
      "outputs": [],
      "source": [
        "!pip install quixstreams==2.3.1 influxdb3-python faker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_vjVIAh9rLl"
      },
      "source": [
        "### 3. Download and setup Kafka and Zookeeper instances\n",
        "\n",
        "Using the default configurations (provided by Apache Kafka) for spinning up the instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4R_5Q3FWtdS"
      },
      "outputs": [],
      "source": [
        "!curl -sSOL https://dlcdn.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz\n",
        "!tar -xzf kafka_2.13-3.6.1.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uf7NR_UZ9wye"
      },
      "outputs": [],
      "source": [
        "!./kafka_2.13-3.6.1/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-3.6.1/config/zookeeper.properties\n",
        "!./kafka_2.13-3.6.1/bin/kafka-server-start.sh -daemon ./kafka_2.13-3.6.1/config/server.properties\n",
        "!echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n",
        "!sleep 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3SafFuS94p1"
      },
      "source": [
        "### 4. Check that the Kafka Daemons are running\n",
        "\n",
        "Show the running daemon processes by filtering the list for the keyword \"java\" while excluding the grep process itself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZDC2lQP99yp"
      },
      "outputs": [],
      "source": [
        "!ps aux | grep -E '[j]ava'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Clone the tutorial repository\n",
        "\n",
        "Get the files you'll need to run in this notebook."
      ],
      "metadata": {
        "id": "qwPh6meOXxCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/quixio/template-influxdbv3-downsampling.git\n",
        "os.chdir(\"template-influxdbv3-downsampling\")\n",
        "!git checkout dev"
      ],
      "metadata": {
        "id": "TmG_1SwwNSDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Rename all the files so that we can distinguish them better\n",
        "\n",
        "This step is just for running the tutorial in Colab. Since we will be running multiple processes in the background, they are harder to manage when they are all called `main.py`. We want to be able to look at the running processes and stop any that are misbehaving."
      ],
      "metadata": {
        "id": "HHqEKM9O36mZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If \"main.py\" exists, rename it so that we can better identify it in Google Colab's process list:\n",
        "if os.path.exists('/content/template-influxdbv3-downsampling/Machine data to InfluxDB/main.py'):\n",
        "    os.rename(\n",
        "        '/content/template-influxdbv3-downsampling/Machine data to InfluxDB/main.py',\n",
        "        '/content/template-influxdbv3-downsampling/Machine data to InfluxDB/machine_generator_main.py'\n",
        "        )\n",
        "\n",
        "if os.path.exists('/content/template-influxdbv3-downsampling/InfluxDB V3 Data Source/main.py'):\n",
        "    os.rename(\n",
        "        '/content/template-influxdbv3-downsampling/InfluxDB V3 Data Source/main.py',\n",
        "        '/content/template-influxdbv3-downsampling/InfluxDB V3 Data Source/influx_reader_main.py'\n",
        "        )\n",
        "\n",
        "if os.path.exists('/content/template-influxdbv3-downsampling/Downsampler/downsampler_main.py'):\n",
        "    os.rename(\n",
        "        '/content/template-influxdbv3-downsampling/Downsampler/main.py',\n",
        "        '/content/template-influxdbv3-downsampling/Downsampler/downsampler_main.py'\n",
        "        )\n",
        "\n",
        "if os.path.exists('/content/template-influxdbv3-downsampling/InfluxDB V3 Data Sink/main.py'):\n",
        "    os.rename(\n",
        "        '/content/template-influxdbv3-downsampling/InfluxDB V3 Data Sink/main.py',\n",
        "        '/content/template-influxdbv3-downsampling/InfluxDB V3 Data Sink/influxwriter_main.py'\n",
        "        )"
      ],
      "metadata": {
        "id": "xvPbwJy03VTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Set some environment variables\n",
        "\n",
        "Set variables required by the different processes that you will run."
      ],
      "metadata": {
        "id": "Zrrnq97xQSyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Variables needed only in this notebook\n",
        "source_db_name = \"machine-generator-data\"\n",
        "dest_db_name = \"machine-generator-downsampled\"\n",
        "raw_data_topic_name = source_db_name + \"_topic\"\n",
        "downsampled_data_topic_name = dest_db_name + \"_topic\"\n",
        "\n",
        "# Kafka-related variables\n",
        "os.environ['BROKER_ADDRESS'] = \"localhost:9092\"\n",
        "os.environ['CONSUMER_GROUP_NAME'] = \"influx-raw-data-consumer\"\n",
        "\n",
        "# Application-specific variables\n",
        "os.environ['task_interval'] = \"30s\"\n",
        "os.environ['target_field'] = \"temperature\"\n",
        "\n",
        "# InfluxDB related variables\n",
        "os.environ['INFLUXDB_ORG'] = \"ContentSquad\"\n",
        "os.environ['INFLUXDB_HOST'] = \"https://us-east-1-1.aws.cloud2.influxdata.com/\"\n",
        "os.environ['INFLUXDB_TAG_KEYS'] = \"['machineID','barcode','provider']\"\n",
        "os.environ['INFLUXDB_FIELD_KEYS'] = \"['temperature','load','power','vibration']\"\n"
      ],
      "metadata": {
        "id": "yWS_1dPlSU5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set your secret InfluxDB token in a way that does not expose it to Google Colab"
      ],
      "metadata": {
        "id": "X0APoSMNTQe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "os.environ['INFLUXDB_TOKEN'] = getpass('Enter your InfluxDB Token: ')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QaM_OyAQhwv",
        "outputId": "8ba65e9e-02f7-4440-8bf6-d0acd114dce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your InfluxDB Token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Generate synthetic machine data (Optional)"
      ],
      "metadata": {
        "id": "4SsL4A1wuFl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step generates some artificial machine sensor data and writes it into your own InfluxDB database.\n",
        "\n",
        "* Follow this step if you don't have existing data that you want to downsample, or want to work with test data first.\n",
        "* You can find the source code at the bottom of this section or [in GitHub](https://github.com/quixio/template-influxdbv3-downsampling/blob/dev/Machine%20data%20to%20InfluxDB/main.py)\n",
        "\n",
        "## Run the code"
      ],
      "metadata": {
        "id": "pbIm6Rse44dx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. Define the InfluxDB database and measurement that should store the raw data from the machine generator."
      ],
      "metadata": {
        "id": "no1FRKJhjnVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['INFLUXDB_DATABASE'] = source_db_name # 'source_db_name' is defined further up in the setup section\n",
        "os.environ['INFLUXDB_MEASUREMENT_NAME'] = \"machine-data-colab\""
      ],
      "metadata": {
        "id": "Pof0zuzVOE-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2. Run the machine generator process in the background (so that we can run subsequent cells)"
      ],
      "metadata": {
        "id": "59dClCHhjWq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup python -u '/content/template-influxdbv3-downsampling/Machine data to InfluxDB/machine_generator_main.py' > machine_generator.log 2>&1 &"
      ],
      "metadata": {
        "id": "DCEjIoxd8ahw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3. Confirm that the process is running:"
      ],
      "metadata": {
        "id": "qt4FyuGUiBMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ps auxww | grep \"[m]achine_generator_main\""
      ],
      "metadata": {
        "id": "QxBdoyBLg8yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4. Check the process logs to make sure there were no errors."
      ],
      "metadata": {
        "id": "5QpVpwYlU6FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat '/content/template-influxdbv3-downsampling/Machine data to InfluxDB/machine_generator.log'\n"
      ],
      "metadata": {
        "id": "s7j9zvokU29L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5. (Optional) If there's a problem with the process, kill it by searching for it by name<br>(i.e. in case that you have decided that it has generated enough data)"
      ],
      "metadata": {
        "id": "EcsF-Gj7iYSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pgrep -f '[m]achine_generator_main' | grep -v grep | wc -l | xargs -I {} bash -c 'if [ {} -eq 1 ]; then pgrep -f \"[m]achine_generator_main\" | xargs kill; fi'\n",
        "print(f\"Killed the 'machine_generator_main.py' process\")"
      ],
      "metadata": {
        "id": "liE5hQil8V1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### See the full machine generator source code\n",
        "\n",
        "Expand this section to see the full source code of the machine generator:\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "j_5deNVjOyNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a InfluxDBClient3 Application\n",
        "write_client = InfluxDBClient3(host=os.environ['INFLUXDB_HOST'], token=os.environ['INFLUXDB_TOKEN'], org=os.environ['INFLUXDB_ORG'])\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "machine_id_counter = 1\n",
        "# Start timestamp\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "# Current time for the loop\n",
        "measurement_name = os.environ['INFLUXDB_MEASUREMENT_NAME']\n",
        "database = os.environ[\"INFLUXDB_DATABASE\"]\n",
        "\n",
        "class machine():\n",
        "    def __init__(self) -> None:\n",
        "        global machine_id_counter\n",
        "        self.machine_id = \"machine\" + str(machine_id_counter)\n",
        "        machine_id_counter += 1\n",
        "        self.temperature = 0\n",
        "        self.load = 0\n",
        "        self.power = 0\n",
        "        self.vibration = 0\n",
        "        self.barcode = fake.ean(length=8)\n",
        "        self.provider = fake.company()\n",
        "        self.fault = False\n",
        "        self.previous_fault_state = False\n",
        "\n",
        "    def toggle_fault(self):\n",
        "        self.previous_fault_state = self.fault\n",
        "        self.fault = not self.fault\n",
        "\n",
        "    def returnMachineID(self):\n",
        "        return self.machine_id\n",
        "\n",
        "    def returnTemperature(self):\n",
        "        currentLoad = self.load\n",
        "        if currentLoad >= 190:\n",
        "            self.temperature = randint(95, 120)\n",
        "        elif currentLoad > 110:\n",
        "            self.temperature = randint(80, 90)\n",
        "        elif currentLoad >= 40:\n",
        "            self.temperature = randint(35, 40)\n",
        "        elif currentLoad > 0:\n",
        "            self.temperature = randint(29, 34)\n",
        "        else:\n",
        "            self.temperature = 20\n",
        "        return self.temperature\n",
        "\n",
        "    def setLoad(self, load):\n",
        "        # TODO dont randomise\n",
        "        self.load = load\n",
        "\n",
        "    def returnPower(self):\n",
        "        currentLoad = self.load\n",
        "        if currentLoad >= 190:\n",
        "            self.power = randint(400, 500)\n",
        "        elif currentLoad > 110:\n",
        "            self.power = randint(300, 320)\n",
        "        elif currentLoad >= 40:\n",
        "            self.power = randint(200, 220)\n",
        "        elif currentLoad == 0:\n",
        "            self.power = 0\n",
        "        else:\n",
        "            self.power = randint(180, 199)\n",
        "\n",
        "        return self.power\n",
        "\n",
        "    def returnVibration(self):\n",
        "        currentLoad = self.load\n",
        "        if currentLoad >= 190:\n",
        "            self.vibration = randint(500, 600)\n",
        "        elif currentLoad > 110:\n",
        "            self.vibration = randint(300, 500)\n",
        "        elif currentLoad == 0:\n",
        "            self.vibration = 0\n",
        "        elif currentLoad >= 40:\n",
        "            self.vibration = randint(80, 90)\n",
        "        else:\n",
        "            self.vibration = randint(50, 79)\n",
        "        return self.vibration\n",
        "\n",
        "    def returnMachineHealth(self):\n",
        "        # trigger load first as needs to be constent:\n",
        "        return {\"metadata\": {\"machineID\": self.returnMachineID(),\n",
        "                             \"barcode\": self.barcode, \"provider\": self.provider},\n",
        "                \"data\": [{\"temperature\": self.returnTemperature()},\n",
        "                         {\"load\": self.load},\n",
        "                         {\"power\": self.returnPower()},\n",
        "                         {\"vibration\": self.returnVibration()}]}\n",
        "\n",
        "\n",
        "def runMachine(m):\n",
        "    counter = 0\n",
        "    counter2 = 0\n",
        "\n",
        "    sleeptime = 0.25\n",
        "    m.setLoad(randint(10, 50))\n",
        "    increasing = True\n",
        "\n",
        "    while True:\n",
        "\n",
        "        # Check if fault state has changed from True to False\n",
        "        if m.previous_fault_state and not m.fault:\n",
        "            m.setLoad(50)\n",
        "            m.previous_fault_state = False\n",
        "\n",
        "        # Chance of fault\n",
        "        if m.fault:\n",
        "            if counter2 == 5:\n",
        "                current_load = m.load\n",
        "                if current_load < 200:\n",
        "                    new_load = min(current_load + 20, 200)\n",
        "                    m.setLoad(new_load)\n",
        "                counter2 = 0\n",
        "        else:\n",
        "            if counter2 == 5:\n",
        "                # Gradually change load between 50 and 99\n",
        "                current_load = m.load\n",
        "                if increasing:\n",
        "                    new_load = current_load + 5\n",
        "                    if new_load >= 99:\n",
        "                        increasing = False\n",
        "                else:\n",
        "                    new_load = current_load - 5\n",
        "                    if new_load <= 50:\n",
        "                        increasing = True\n",
        "\n",
        "                m.setLoad(new_load)\n",
        "                counter2 = 0\n",
        "\n",
        "        check_machine = m.returnMachineHealth()\n",
        "        timestamp_str = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
        "\n",
        "        point = {\n",
        "            \"measurement\": measurement_name,\n",
        "            \"tags\": {\n",
        "                'machineId': check_machine['metadata']['machineID'],\n",
        "                'barcode': check_machine['metadata']['barcode'],\n",
        "                'provider': check_machine['metadata']['provider'],\n",
        "            },\n",
        "            \"fields\": {\n",
        "                'temperature': check_machine['data'][0]['temperature'],\n",
        "                'load': check_machine['data'][1]['load'],\n",
        "                'power': check_machine['data'][2]['power'],\n",
        "                'vibration': check_machine['data'][3]['vibration'],\n",
        "            },\n",
        "            \"time\": timestamp_str\n",
        "        }\n",
        "\n",
        "        write_client.write(database=database, record=point)\n",
        "\n",
        "        print(f\"Wrote point {point}\")\n",
        "\n",
        "        sleep(sleeptime)\n",
        "        counter = counter + 1\n",
        "        counter2 = counter2 + 1\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Initialize a machine instance\n",
        "    m = machine()\n",
        "\n",
        "    # Start running the machine to produce data\n",
        "    runMachine(m)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Exiting.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wfeKHubxMsOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br><br>"
      ],
      "metadata": {
        "id": "UNXSXY3syT63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Read the raw data from InfluxDB\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2Xxld7YhLkyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, you will run a process that reads the raw data from your source InfluxDB database and sends it to a Kafka topic for the downsampling process to consume.\n",
        "\n",
        "* You can find the source code at the bottom of this section or [in GitHub](https://github.com/quixio/template-influxdbv3-downsampling/blob/dev/InfluxDB%20V3%20Data%20Source/main.py)\n",
        "\n",
        "## Summary of what this code does\n",
        "\n",
        "First it uses the Quix library's `Application` class to initialize a connection to the local Apache Kafka.\n",
        "\n",
        "```python\n",
        "...\n",
        "app = Application(\n",
        "  broker_address=os.environ.get('BROKER_ADDRESS','localhost:9092'),\n",
        "  consumer_group=consumer_group_name,\n",
        "  auto_create_topics=True\n",
        ")\n",
        "...\n",
        "```\n",
        "\n",
        "It then uses the InfluxDB clients query feature to get data from the source database.\n",
        "\n",
        "```python\n",
        "...\n",
        "myquery = f'SELECT * FROM \"{measurement_name}\" WHERE time >= {interval}'\n",
        "table = influxdb3_client.query(\n",
        "                        query=myquery,\n",
        "                        mode=\"pandas\",\n",
        "                        language=\"influxql\")\n",
        "...\n",
        "```\n",
        "\n",
        "This query runs at regular intervals, as configured in the `task_interval` setting. This setting defines both how often the query should be run agains the database and the max age of the data to return.\n",
        "\n",
        "Finally, the results of the query are sent to Kafka using the Quixstreams `get_producer()` method.\n",
        "\n",
        "```python\n",
        "...\n",
        "message_key = obj['machineId']\n",
        "logger.info(f\"Produced message with key:{message_key}, value:{obj}\")\n",
        "\n",
        "serialized = topic.serialize(\n",
        "    key=message_key, value=obj, headers={\"uuid\": str(uuid.uuid4())}\n",
        "    )\n",
        "\n",
        "# publish the data to the topic\n",
        "producer.produce(\n",
        "    topic=topic.name,\n",
        "    headers=serialized.headers,\n",
        "    key=serialized.key,\n",
        "    value=serialized.value,\n",
        ")\n",
        "...\n",
        "```\n"
      ],
      "metadata": {
        "id": "CNmrZXnT4wf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Run the code\n",
        "\n",
        " 1. Define the name of the Kafka topic that should receive the raw data and the measurement to use in the query."
      ],
      "metadata": {
        "id": "Y1lWJlF4mG8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['output'] = raw_data_topic_name\n",
        "os.environ['localdev'] = \"true\" # Set this so that Quixstreams looks for a local Kafka\n",
        "os.environ['INFLUXDB_MEASUREMENT_NAME'] = \"machine-data-colab\" # Change this if you already have another measurement that you want to downsample"
      ],
      "metadata": {
        "id": "FvAbkhZSPEtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2. Run the influx reader process in the background (so that we can run subsequent cells)"
      ],
      "metadata": {
        "id": "5Cguj_hDnaGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup python -u '/content/template-influxdbv3-downsampling/InfluxDB V3 Data Source/influx_reader_main.py' > '/content/template-influxdbv3-downsampling/InfluxDB V3 Data Source/influx_reader.log' 2>&1 &"
      ],
      "metadata": {
        "id": "yWXdk7fn87A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3. Confirm that the process is running:"
      ],
      "metadata": {
        "id": "P5PvUrPbnxi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ps auxww | grep '[i]nflux_reader_main'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfHHnYZ9_epw",
        "outputId": "2ae4cf05-98f0-44db-f66f-8dab49c2729d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       43598  8.1  1.2 1581092 164060 ?      Sl   11:31   0:03 python3 -u influx_consumer_main.p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4. Check the process logs to make sure there were no errors."
      ],
      "metadata": {
        "id": "solLvbgyn6m1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat '/content/template-influxdbv3-downsampling/InfluxDB V3 Data Source/influx_reader.log'"
      ],
      "metadata": {
        "id": "2XRo0NJH9IAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5. (Optional) If there's a problem with the process, kill it by searching for it by name<br>(i.e. in case it has a problem but does not terminate by default)"
      ],
      "metadata": {
        "id": "txs0WgULoAyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pgrep -f '[i]nflux_reader_main' | grep -v grep | wc -l | xargs -I {} bash -c 'if [ {} -eq 1 ]; then pgrep -f \"[i]nflux_reader_main\" | xargs kill; fi'\n",
        "print(f\"Killed the 'influx_reader_main' process\")"
      ],
      "metadata": {
        "id": "cHEDXRZcpzuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### See the full InfluxDB reader source code"
      ],
      "metadata": {
        "id": "wXiebZOoO8G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Create an Application\n",
        "app = Application.Quix(consumer_group=\"influxdbv3_source\", auto_create_topics=True)\n",
        "\n",
        "# Define a serializer for messages, using JSON Serializer for ease\n",
        "serializer = JSONSerializer()\n",
        "\n",
        "# Define the topic using the \"output\" environment variable\n",
        "topic_name = os.environ[\"output\"]\n",
        "topic = app.topic(name=topic_name, value_serializer=\"json\")\n",
        "\n",
        "influxdb3_client = InfluxDBClient3.InfluxDBClient3(token=os.environ[\"INFLUXDB_TOKEN\"],\n",
        "                         host=os.environ[\"INFLUXDB_HOST\"],\n",
        "                         org=os.environ[\"INFLUXDB_ORG\"],\n",
        "                         database=os.environ[\"INFLUXDB_DATABASE\"])\n",
        "\n",
        "measurement_name = os.environ.get(\"INFLUXDB_MEASUREMENT_NAME\", os.environ[\"output\"])\n",
        "interval = os.environ.get(\"task_interval\", \"5m\")\n",
        "\n",
        "# should the main loop run?\n",
        "# Global variable to control the main loop's execution\n",
        "run = True\n",
        "\n",
        "# Helper function to convert time intervals (like 1h, 2m) into seconds for easier processing.\n",
        "# This function is useful for determining the frequency of certain operations.\n",
        "UNIT_SECONDS = {\n",
        "    \"s\": 1,\n",
        "    \"m\": 60,\n",
        "    \"h\": 3600,\n",
        "    \"d\": 86400,\n",
        "    \"w\": 604800,\n",
        "    \"y\": 31536000,\n",
        "}\n",
        "\n",
        "def interval_to_seconds(interval: str) -> int:\n",
        "    try:\n",
        "        return int(interval[:-1]) * UNIT_SECONDS[interval[-1]]\n",
        "    except ValueError as e:\n",
        "        if \"invalid literal\" in str(e):\n",
        "            raise ValueError(\n",
        "                \"interval format is {int}{unit} i.e. '10h'; \"\n",
        "                f\"valid units: {list(UNIT_SECONDS.keys())}\")\n",
        "    except KeyError:\n",
        "        raise ValueError(\n",
        "            f\"Unknown interval unit: {interval[-1]}; \"\n",
        "            f\"valid units: {list(UNIT_SECONDS.keys())}\")\n",
        "\n",
        "interval_seconds = interval_to_seconds(interval)\n",
        "\n",
        "# Function to fetch data from InfluxDB and send it to Quix\n",
        "# It runs in a continuous loop, periodically fetching data based on the interval.\n",
        "def get_data():\n",
        "    # Run in a loop until the main thread is terminated\n",
        "    while run:\n",
        "        try:\n",
        "            myquery = f'SELECT * FROM \"{measurement_name}\" WHERE time >= {interval}'\n",
        "            print(f\"sending query {myquery}\")\n",
        "            # Query InfluxDB 3.0 using influxql or sql\n",
        "            table = influxdb3_client.query(\n",
        "                                    query=myquery,\n",
        "                                    mode=\"pandas\",\n",
        "                                    language=\"influxql\")\n",
        "\n",
        "            table = table.drop(columns=[\"iox::measurement\"])\n",
        "            table.rename(columns={'time': 'time_recorded'}, inplace=True)\n",
        "            # If there are rows to write to the stream at this time\n",
        "            if not table.empty:\n",
        "                json_result = table.to_json(orient='records', date_format='iso')\n",
        "                yield json_result\n",
        "                print(\"query success\")\n",
        "            else:\n",
        "                print(\"No new data to publish.\")\n",
        "\n",
        "            # Wait for the next interval\n",
        "            sleep(interval_seconds)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"query failed\", flush=True)\n",
        "            print(f\"error: {e}\", flush=True)\n",
        "            sleep(1)\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Read data from the Query and publish it to Kafka\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a pre-configured Producer object.\n",
        "    # Producer is already setup to use Quix brokers.\n",
        "    # It will also ensure that the topics exist before producing to them if\n",
        "    # Application.Quix is initialized with \"auto_create_topics=True\".\n",
        "\n",
        "    with app.get_producer() as producer:\n",
        "        for res in get_data():\n",
        "            # Parse the JSON string into a Python object\n",
        "            records = json.loads(res)\n",
        "            for index, obj in enumerate(records):\n",
        "                print(obj)\n",
        "                # Generate a unique message_key for each row\n",
        "                message_key = obj['machineId']\n",
        "                logger.info(f\"Produced message with key:{message_key}, value:{obj}\")\n",
        "\n",
        "                serialized = topic.serialize(\n",
        "                    key=message_key, value=obj, headers={\"uuid\": str(uuid.uuid4())}\n",
        "                    )\n",
        "\n",
        "                # publish the data to the topic\n",
        "                producer.produce(\n",
        "                    topic=topic.name,\n",
        "                    headers=serialized.headers,\n",
        "                    key=serialized.key,\n",
        "                    value=serialized.value,\n",
        "                )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Exiting.\")"
      ],
      "metadata": {
        "id": "QgoXBJQbLtbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Downsample the data"
      ],
      "metadata": {
        "id": "flGqD8RdL0C2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, you will run a process that reads the raw data from the input Kafka, downsamples it, the sends it to an output topic.\n",
        "\n",
        "This is where the Quixstreams library excels—processing high volumes of data in real time.\n",
        "\n",
        "* You can find the source code at the bottom of this section or [in GitHub](https://github.com/quixio/template-influxdbv3-downsampling/blob/dev/Downsampler/main.py)\n",
        "\n",
        "## Summary of what this code does\n",
        "\n",
        "As before, it uses the Quixstreams library's `Application` class to initialize a connection to our local Apache Kafka.\n",
        "\n",
        "It then uses the in-built windowing feature to create a tumbling window to downsample the data into 1-minute buckets.\n",
        "\n",
        "```python\n",
        "...\n",
        "target_field = os.environ['target_field']  # By default this is \"temperature\"\n",
        "\n",
        "def custom_ts_extractor(value):\n",
        "    # ... custom code that defines the 'time_recorded' field as the timestamp to use for windowing...\n",
        "\n",
        "topic = app.topic(input_topic, timestamp_extractor=custom_ts_extractor)\n",
        "\n",
        "sdf = (\n",
        "    sdf.apply(lambda value: value[target_field])  # Extract temperature values\n",
        "    .tumbling_window(timedelta(minutes=1))   # 1-minute tumbling windows\n",
        "    .mean()                                  # Calculate average temperature\n",
        "    .final()                                 # Emit results at window completion\n",
        ")\n",
        "\n",
        "sdf = sdf.apply(\n",
        "    lambda value: {\n",
        "        \"time\": value[\"end\"],                  # End of the window\n",
        "        \"temperature_avg\": value[\"value\"],     # Average temperature\n",
        "    }\n",
        ")\n",
        "\n",
        "sdf.to_topic(output_topic)\n",
        "...\n",
        "```\n",
        "Explanation:\n",
        "* **custom_ts_extractor**: ensures the downsampling uses the `time_recorded` field in the data so that we  have more control over the behaviour of time-based windows (otherwise it uses the Kafka message timestamp).\n",
        "* **tumbling_window**: ensures that the data is grouped into non-overlapping 1-minute windows.\n",
        "* **Aggregation—mean()**: Calculates the average temperature within each window.\n",
        "* **Output Formatting**: Transforms the result into a format with time (window end) and the final windowed value `temperature_avg`.\n",
        "* **Sending to Kafka**: `sdf.to_topic(output_topic)` sends downsampled results to the Kafka topic defined in the `output_topic` variable.\n",
        "\n",
        "Note: \"sdf\" stands for \"Streaming Dataframe\".\n"
      ],
      "metadata": {
        "id": "jlDJZKrT5HZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Run the code\n",
        "\n",
        " 1. Define the name of the Kafka topic that should receive the downsampled data."
      ],
      "metadata": {
        "id": "dDp9bItNoz97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['input'] = raw_data_topic_name\n",
        "os.environ['output'] = downsampled_data_topic_name"
      ],
      "metadata": {
        "id": "NV32Xhk6PTYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2. Run the downsampling process in the background (so that we can run subsequent cells)"
      ],
      "metadata": {
        "id": "TAQLx7vHpdEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup python -u '/content/template-influxdbv3-downsampling/Downsampler/downsampler_main.py' > '/content/template-influxdbv3-downsampling/Downsampler/downsampler.log' 2>&1 &"
      ],
      "metadata": {
        "id": "tTgU8bO-9YQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3. Confirm that the process is running:"
      ],
      "metadata": {
        "id": "ZWSRsiVcpmdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ps auxww | grep '[d]ownsampler_main'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KpOsIS9-iSb",
        "outputId": "e0d71ed5-c051-48aa-acd4-2b629c4c6b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       11740  3.0  0.3 1229080 47644 ?       Sl   09:19   0:01 python3 -u downsampler_main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4. Check the process logs to make sure there were no errors."
      ],
      "metadata": {
        "id": "14EN9Dk_pqUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat '/content/template-influxdbv3-downsampling/Downsampler/downsampler.log'"
      ],
      "metadata": {
        "id": "FrYDLild9aDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5. (Optional) If there's a problem with the process, kill it by searching for it by name.<br>(i.e. in case it has a problem but does not terminate by default)"
      ],
      "metadata": {
        "id": "Do_ruOJ6pt5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pgrep -f '[d]ownsampler_main.py' | grep -v grep | wc -l | xargs -I {} bash -c 'if [ {} -eq 1 ]; then pgrep -f \"[d]ownsampler_main.py\" | xargs kill; fi'\n",
        "print(f\"Killed the 'downsampler_main.py' process\")"
      ],
      "metadata": {
        "id": "4ikH_8vMqAdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### See the full downsampler source code"
      ],
      "metadata": {
        "id": "Zp-Ob4ZTPPLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from quixstreams import Application\n",
        "from quixstreams.models.serializers.quix import JSONDeserializer, JSONSerializer\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "app = Application.Quix(consumer_group=\"downsampling-consumer-groupv6\", auto_offset_reset=\"earliest\")\n",
        "input_topic = app.topic(os.environ[\"input\"], value_deserializer=JSONDeserializer())\n",
        "output_topic = app.topic(os.environ[\"output\"], value_serializer=JSONSerializer())\n",
        "\n",
        "data_key = os.environ[\"data_key\"]\n",
        "logger.info(f\"Data key is: {data_key}\")\n",
        "\n",
        "sdf = app.dataframe(input_topic)\n",
        "sdf = sdf.update(lambda value: logger.info(f\"Input value received: {value}\"))\n",
        "\n",
        "def custom_ts_extractor(value):\n",
        "    \"\"\"\n",
        "    Specifying a custom timestamp extractor to use the timestamp from the message payload\n",
        "    instead of Kafka timestamp.\n",
        "    \"\"\"\n",
        "    # Convert to a datetime object\n",
        "    dt_obj = datetime.strptime(value[\"time_recorded\"], \"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "\n",
        "    # Convert to milliseconds since the Unix epoch\n",
        "    milliseconds = int(dt_obj.timestamp() * 1000)\n",
        "    value[\"timestamp\"] = milliseconds\n",
        "    logger.info(f\"Value of new timestamp is: {value['timestamp']}\")\n",
        "    return value[\"timestamp\"]\n",
        "\n",
        "# Passing the timestamp extractor to the topic.\n",
        "\n",
        "# The window functions will now use the extracted timestamp instead of the Kafka timestamp.\n",
        "topic = app.topic(\"input-topic\", timestamp_extractor=custom_ts_extractor)\n",
        "\n",
        "sdf = (\n",
        "    # Extract the relevant field from the record\n",
        "    sdf.apply(lambda value: value[data_key])\n",
        "\n",
        "    # Define a tumbling window of 1 minute\n",
        "    .tumbling_window(timedelta(minutes=1))\n",
        "\n",
        "    # Specify the \"mean\" aggregation function to apply to values of the data key\n",
        "    .mean()\n",
        "\n",
        "    # Emit results only when the 1 minute window has elapsed\n",
        "    .final()\n",
        "    #.current() #for debug purposes.\n",
        ")\n",
        "\n",
        "sdf = sdf.apply(\n",
        "    lambda value: {\n",
        "        \"time\": value[\"end\"],\n",
        "        f\"{data_key}\": value[\"value\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "# Produce the result to the output topic\n",
        "sdf = sdf.to_topic(output_topic)\n",
        "sdf = sdf.update(lambda value: logger.info(f\"Produced value: {value}\"))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting application\")\n",
        "    app.run(sdf)"
      ],
      "metadata": {
        "id": "f3B8Blm5MFld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Insert the downsampled data back into InfluxDB"
      ],
      "metadata": {
        "id": "Ra0ZgbOyL52d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. Define the name of the Kafka topic from which to read the downsampled data.<br>Also:\n",
        "  * Set the name of the destination bucket.\n",
        "  * Set the name of the measurement to use in the destination bucket."
      ],
      "metadata": {
        "id": "oJ9rj2rPrik7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['input'] = downsampled_data_topic_name\n",
        "os.environ['INFLUXDB_DATABASE'] = dest_db_name\n",
        "os.environ['INFLUXDB_MEASUREMENT_NAME'] = \"machine-data-downsampled-colab\""
      ],
      "metadata": {
        "id": "0DoMCYVjQGNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Run the InfluxDB write process in the background (so that we can run subsequent cells)"
      ],
      "metadata": {
        "id": "lNlZt-r9sxiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup python -u '/content/template-influxdbv3-downsampling/InfluxDB V3 Data Sink/influxwriter_main.py' > '/content/template-influxdbv3-downsampling/InfluxDB V3 Data Sink/influxwriter.log' 2>&1 &"
      ],
      "metadata": {
        "id": "jqC2PcWJBTV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Confirm that the process is running:\n"
      ],
      "metadata": {
        "id": "8awZtNF4tjXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux | grep '[i]nfluxwriter_main'"
      ],
      "metadata": {
        "id": "cT4LZZFeBfPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Check the process logs to make sure there were no errors."
      ],
      "metadata": {
        "id": "fAOenrQitq1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat '/content/template-influxdbv3-downsampling/InfluxDB V3 Data Sink/influxwriter.log'"
      ],
      "metadata": {
        "id": "yHX7znOSBYlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. (optional) If there's a problem with the process, kill it by searching for it by name:"
      ],
      "metadata": {
        "id": "rZmoenMFspuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pgrep -f '[i]nfluxwriter_main' | grep -v grep | wc -l | xargs -I {} bash -c 'if [ {} -eq 1 ]; then pgrep -f \"[i]nfluxwriter_main\" | xargs kill; fi'"
      ],
      "metadata": {
        "id": "rBjK-sfnS39s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## See the full InfluxDB writer source code"
      ],
      "metadata": {
        "id": "LrZ8hs81P9eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import vendor-specific modules\n",
        "from quixstreams import Application\n",
        "from quixstreams.models.serializers.quix import JSONDeserializer\n",
        "from influxdb_client_3 import InfluxDBClient3\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "consumer_group_name = os.environ.get('CONSUMER_GROUP_NAME', \"influxdb-data-writer\")\n",
        "\n",
        "app = Application.Quix(consumer_group=consumer_group_name,\n",
        "                       auto_offset_reset=\"earliest\")\n",
        "\n",
        "input_topic = app.topic(os.environ[\"input\"], value_deserializer=JSONDeserializer())\n",
        "\n",
        "\n",
        "# Read the environment variable and convert it to a dictionary\n",
        "tag_keys = ast.literal_eval(os.environ.get('INFLUXDB_TAG_KEYS', \"[]\"))\n",
        "field_keys = ast.literal_eval(os.environ.get('INFLUXDB_FIELD_KEYS', \"[]\"))\n",
        "\n",
        "\n",
        "# Read the environment variable for the field(s) to get.\n",
        "# For multiple fields, use a list \"['field1','field2']\"\n",
        "\n",
        "influx3_client = InfluxDBClient3(token=os.environ[\"INFLUXDB_TOKEN\"],\n",
        "                         host=os.environ[\"INFLUXDB_HOST\"],\n",
        "                         org=os.environ[\"INFLUXDB_ORG\"],\n",
        "                         database=os.environ[\"INFLUXDB_DATABASE\"])\n",
        "\n",
        "def send_data_to_influx(message):\n",
        "    logger.info(f\"Processing message: {message}\")\n",
        "    try:\n",
        "        # Uses the current time as the timestamp for writing to the sink\n",
        "        # Adjust to use an alternative timestamp if necesssary,\n",
        "\n",
        "        writetime = datetime.datetime.utcnow()\n",
        "        writetime = writetime.isoformat(timespec='milliseconds') + 'Z'\n",
        "\n",
        "        measurement_name = os.environ.get('INFLUXDB_MEASUREMENT_NAME', \"measurement1\")\n",
        "\n",
        "        # Initialize the tags and fields dictionaries\n",
        "        tags = {}\n",
        "        fields = {}\n",
        "\n",
        "        # Iterate over the tag_dict and field_dict to populate tags and fields\n",
        "        for tag_key in tag_keys:\n",
        "            if tag_key in message:\n",
        "                tags[tag_key] = message[tag_key]\n",
        "\n",
        "        for field_key in field_keys:\n",
        "            if field_key in message:\n",
        "                fields[field_key] = message[field_key]\n",
        "\n",
        "        logger.info(f\"Using tag keys: {', '.join(tags.keys())}\")\n",
        "        logger.info(f\"Using field keys: {', '.join(fields.keys())}\")\n",
        "\n",
        "        # Construct the points dictionary\n",
        "        points = {\n",
        "            \"measurement\": measurement_name,\n",
        "            \"tags\": tags,\n",
        "            \"fields\": fields,\n",
        "            \"time\": message['time']\n",
        "        }\n",
        "\n",
        "        influx3_client.write(record=points, write_precision=\"ms\")\n",
        "\n",
        "        print(f\"{str(datetime.datetime.utcnow())}: Persisted ponts to influx: {points}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{str(datetime.datetime.utcnow())}: Write failed\")\n",
        "        print(e)\n",
        "\n",
        "sdf = app.dataframe(input_topic)\n",
        "sdf = sdf.update(send_data_to_influx)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting application\")\n",
        "    app.run(sdf)"
      ],
      "metadata": {
        "id": "kowrwCpUMUAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDpKDmKlluED"
      },
      "source": [
        "### Run a test query on the destination\n",
        "\n",
        "Use Qdrant to do a basic similarity seach to make sure the vectors have been ingested properly and are matching in the expected way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-Sep1MUlzMj",
        "outputId": "0bacc16a-1ccd-4741-c6f8-46019d32fa27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entries matching your query:\n",
            "Old Man's War  |  Earth's senior citizens are recruited to fight in an interstellar war, discovering new alien cultures and threats. score: 0.29142217809948834\n",
            "Dune  |  A desert planet is the site of political intrigue and power struggles. score: 0.2174839673349857\n",
            "Footfall  |  Elephant-like aliens invade Earth, and humanity must find a way to fight back. score: 0.18069611904932592\n",
            "Foundation  |  A mathematician develops a science to predict the future of humanity and works to save civilization from collapse. score: 0.16798619628421962\n",
            "Contact  |  Scientists receive a message from extraterrestrial beings and build a machine to meet them. score: 0.1639943016350539\n",
            "The Forge of God  |  Aliens arrive under the guise of friendship, but their true mission is to destroy Earth. score: 0.163921273441558\n",
            "The Hunger Games  |  A dystopian society where teenagers are forced to fight to the death in a televised spectacle. score: 0.16221441071437032\n",
            "Childhood's End  |  A peaceful alien invasion leads to the end of humanity's childhood. score: 0.15964121487784855\n",
            "The Time Traveler's Wife  |  A love story between a man who involuntarily time travels and the woman he loves. score: 0.1485679339315098\n",
            "The Kraken Wakes  |  Alien beings from the depths of the ocean start attacking humanity. score: 0.14585549153746558\n"
          ]
        }
      ],
      "source": [
        "query = \"books like star wars\" # Leave the test query as-is for the first attempt\n",
        "\n",
        "hits = qdrant.search(\n",
        "    collection_name=collectionname,\n",
        "    query_vector=encoder.encode(query).tolist(),\n",
        "    limit=10\n",
        ")\n",
        "\n",
        "print(\"Entries matching your query:\")\n",
        "for hit in hits:\n",
        "  print(hit.payload['doc_name'], \" | \", hit.payload['doc_descr'], \"score:\", hit.score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything went to plan, \"*Dune*\" should be top match for the query \"*books like star wars*\". This makes sense, since Dune is kind of like Star Wars (depending on who you ask). We can guess it matched because planet\" is semantically close to \"star\" and \"struggles\" is semantically close to \"wars\".\n",
        "\n",
        "Now let's suppose we update our catalog to with more books to acommodate all those who are looking for similar items. We want the vector store to be updated as soon as the new book entries are entered in the main catalog database. This will ensure we get as many good matches (and hopefully purchases) as possible without any delays."
      ],
      "metadata": {
        "id": "lVi7GZBIuWfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jLT0mhhDP7PC"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Jr0T6gYFutcO",
        "4SsL4A1wuFl3",
        "j_5deNVjOyNl",
        "2Xxld7YhLkyH",
        "wXiebZOoO8G8",
        "flGqD8RdL0C2",
        "Zp-Ob4ZTPPLz",
        "Ra0ZgbOyL52d",
        "LrZ8hs81P9eZ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
